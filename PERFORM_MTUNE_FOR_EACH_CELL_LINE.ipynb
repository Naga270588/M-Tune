{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351a7c9d-73de-4ef8-87b2-d565518ee12a",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b222c0c-f0d4-41ab-933e-e762e56ecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Viusalization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Performance evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import \\\n",
    "                accuracy_score, \\\n",
    "                classification_report, \\\n",
    "                cohen_kappa_score, \\\n",
    "                matthews_corrcoef, \\\n",
    "                confusion_matrix, \\\n",
    "                roc_auc_score\n",
    "\n",
    "from sklearn import metrics\n",
    "# Model Saving\n",
    "import pickle,os\n",
    "\n",
    "#  Machine Learning Algorithm libraries\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7773e-bd99-4292-818a-9f3618295960",
   "metadata": {},
   "source": [
    "# Peformance Evalutation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f401a-5346-4d96-b1b5-3ce4bc3ad031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_true,y_pred,pred_prob='NA'):\n",
    "    cr = classification_report(y_true,y_pred)\n",
    "    filtered_cr = [ line for line in cr.split('\\n') if len(line)!=0]\n",
    "\n",
    "    precision_0 = float(filtered_cr[1].split()[1])\n",
    "    recall_0 = float(filtered_cr[1].split()[2])\n",
    "    f1_0 = float(filtered_cr[1].split()[3])\n",
    "    \n",
    "    precision_1 = float(filtered_cr[2].split()[1])\n",
    "    recall_1 = float(filtered_cr[2].split()[2])\n",
    "    f1_1 = float(filtered_cr[2].split()[3])\n",
    "\n",
    "    balanced_accuracy = (recall_0 + recall_1)/2\n",
    "    acc_score = accuracy_score(y_true,y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    matthews_corrcoef_score = matthews_corrcoef(y_true,y_pred)\n",
    "    if pred_prob != 'NA':\n",
    "        roc_auc = roc_auc_score(y_true,pred_prob)\n",
    "    else:\n",
    "        roc_auc = pred_prob\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    temp = dict(\n",
    "        precision_0 = precision_0,\n",
    "        precision_1 = precision_1,\n",
    "        recall_0 = recall_0,\n",
    "        recall_1 = recall_1,\n",
    "        f1_0 = f1_0,\n",
    "        f1_1 = f1_1,\n",
    "        accuracy = acc_score,\n",
    "        balanced_accuracy = balanced_accuracy,\n",
    "        cohen_kappa = cohen_kappa,\n",
    "        matthews_corrcoef_score = matthews_corrcoef_score,\n",
    "        roc_auc_score = roc_auc,\n",
    "        \n",
    "        tn = tn,\n",
    "        fp = fp,\n",
    "        fn = fn,\n",
    "        tp = tp\n",
    "    )\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1a5-0702-4078-bd8e-922cbbb6fff4",
   "metadata": {},
   "source": [
    "# GHOST Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f87df3-f090-4c9b-b0b2-a784b32c5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold_train_subset(cls, fps_train, labels_train, thresholds, \n",
    "                                    ThOpt_metrics = 'Kappa', N_subsets = 100, \n",
    "                                    subsets_size = 0.2, with_replacement = False, random_seed = None):\n",
    "\n",
    "    \"\"\"Optimize the decision threshold based on subsets of the training set.\n",
    "    The threshold that maximizes the Cohen's kappa coefficient or a ROC-based criterion \n",
    "    on the training subsets is chosen as optimal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cls : obj\n",
    "        Trained machine learning classifier built using scikit-learn\n",
    "    fps_train: list \n",
    "        Molecular descriptors for the training set\n",
    "    labels_train: list of int\n",
    "        True labels for the training set\n",
    "    thresholds: list of floats\n",
    "        List of decision thresholds to screen for classification\n",
    "    ThOpt_metrics: str\n",
    "        Optimization metric. Choose between \"Kappa\" and \"ROC\"\n",
    "    N_subsets: int\n",
    "        Number of training subsets to use in the optimization\n",
    "    subsets_size: float or int\n",
    "        Size of the subsets. if float, represents the proportion of the dataset to include in the subsets. \n",
    "        If integer, it represents the actual number of instances to include in the subsets. \n",
    "    with_replacement: bool\n",
    "        The subsets are drawn randomly. True to draw the subsets with replacement\n",
    "    random_seed: int    \n",
    "        random number to seed the drawing of the subsets\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    thresh: float\n",
    "        Optimal decision threshold for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    # seeding\n",
    "    np.random.seed(random_seed)\n",
    "    random_seeds = np.random.randint(N_subsets*10, size=N_subsets)  \n",
    "    \n",
    "    # calculate prediction probability for the training set\n",
    "    probs_train = cls.predict_proba(fps_train)[:,1]\n",
    "    labels_train_thresh = {'labels': labels_train}\n",
    "    labels_train_thresh.update({'probs': probs_train})\n",
    "    # recalculate the predictions for the training set using different thresholds and\n",
    "    # store the predictions in a dataframe\n",
    "    for thresh in thresholds:\n",
    "        labels_train_thresh.update({str(thresh): [1 if x >= thresh else 0 for x in probs_train]})\n",
    "    df_preds = pd.DataFrame(labels_train_thresh)\n",
    "    # Optmize the decision threshold based on the Cohen's Kappa coefficient\n",
    "    if ThOpt_metrics == 'Kappa':\n",
    "        # pick N_subsets training subsets and determine the threshold that provides the highest kappa on each \n",
    "        # of the subsets\n",
    "        kappa_accum = []\n",
    "        for i in range(N_subsets):\n",
    "            if with_replacement:\n",
    "                if isinstance(subsets_size, float):\n",
    "                    Nsamples = int(df_preds.shape[0]*subsets_size)\n",
    "                elif isinstance(subsets_size, int):\n",
    "                    Nsamples = subsets_size                    \n",
    "                df_subset = resample(df_preds, n_samples = Nsamples, stratify=list(df_preds.labels), random_state = random_seeds[i])\n",
    "                labels_subset = df_subset['labels']\n",
    "            else:\n",
    "                df_tmp, df_subset, labels_tmp, labels_subset = train_test_split(df_preds, labels_train, test_size = subsets_size, stratify = labels_train, random_state = random_seeds[i])\n",
    "            probs_subset = list(df_subset['probs'])\n",
    "            thresh_names = [x for x in df_preds.columns if (x != 'labels' and x != 'probs')]\n",
    "            kappa_train_subset = []\n",
    "            for col1 in thresh_names:\n",
    "                kappa_train_subset.append(metrics.cohen_kappa_score(labels_subset, list(df_subset[col1])))\n",
    "            kappa_accum.append(kappa_train_subset)\n",
    "        # determine the threshold that provides the best results on the training subsets\n",
    "        y_values_median, y_values_std = helper_calc_median_std(kappa_accum)\n",
    "        opt_thresh = thresholds[np.argmax(y_values_median)]\n",
    "    # Optmize the decision threshold based on the ROC-curve, as described here https://doi.org/10.1007/s11548-013-0913-8\n",
    "    elif ThOpt_metrics == 'ROC':\n",
    "        sensitivity_accum = []\n",
    "        specificity_accum = []\n",
    "        # Calculate sensitivity and specificity for a range of thresholds and N_subsets\n",
    "        for i in range(N_subsets):\n",
    "            if with_replacement:\n",
    "                if isinstance(subsets_size, float):\n",
    "                    Nsamples = int(df_preds.shape[0]*subsets_size)\n",
    "                elif isinstance(subsets_size, int):\n",
    "                    Nsamples = subsets_size                    \n",
    "                df_subset = resample(df_preds, n_samples = Nsamples, stratify=list(df_preds.labels), random_state = random_seeds[i])\n",
    "                labels_subset = list(df_subset['labels'])\n",
    "            else:\n",
    "                df_tmp, df_subset, labels_tmp, labels_subset = train_test_split(df_preds, labels_train, test_size = subsets_size, stratify = labels_train, random_state = random_seeds[i])\n",
    "            probs_subset = list(df_subset['probs'])\n",
    "            sensitivity = []\n",
    "            specificity = []\n",
    "            for thresh in thresholds:\n",
    "                scores = [1 if x >= thresh else 0 for x in probs_subset]\n",
    "                tn, fp, fn, tp = metrics.confusion_matrix(labels_subset, scores, labels=list(set(labels_train))).ravel()\n",
    "                sensitivity.append(tp/(tp+fn))\n",
    "                specificity.append(tn/(tn+fp))\n",
    "            sensitivity_accum.append(sensitivity)\n",
    "            specificity_accum.append(specificity)\n",
    "        # determine the threshold that provides the best results on the training subsets\n",
    "        median_sensitivity, std_sensitivity = helper_calc_median_std(sensitivity_accum)\n",
    "        median_specificity, std_specificity = helper_calc_median_std(specificity_accum)\n",
    "        roc_dist_01corner = (2*median_sensitivity*median_specificity)/(median_sensitivity+median_specificity)\n",
    "        opt_thresh = thresholds[np.argmax(roc_dist_01corner)]\n",
    "    return opt_thresh\n",
    "\n",
    "\n",
    "def helper_calc_median_std(specificity):\n",
    "    # Calculate median and std of the columns of a pandas dataframe\n",
    "    arr = np.array(specificity)\n",
    "    y_values_median = np.median(arr,axis=0)\n",
    "    y_values_std = np.std(arr,axis=0)\n",
    "    return y_values_median, y_values_std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb276930-2fc1-47a9-ad89-c35b7205b8bd",
   "metadata": {},
   "source": [
    "# M-Tune Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c03b7d-b33a-4197-910a-b39722137b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mtune:\n",
    "    def __init__(self, base_model, method='ensemble', n_ensemble=11,random_state = None):\n",
    "        self.base_model = base_model\n",
    "        self.method = method\n",
    "        self.n_ensemble = n_ensemble\n",
    "        self.threshold = None\n",
    "        self.majority_class = None\n",
    "        self.minority_class = None\n",
    "        self.merged_data = None\n",
    "        self.random_state = None\n",
    "        self.thres_to_model_mapping = dict()\n",
    "        \n",
    "    def fit(self, X, y,verbose='off'):\n",
    "        \n",
    "        class_col_name = y.name\n",
    "        val_counts = y.value_counts()\n",
    "        \n",
    "        self.majority_class = ( val_counts.idxmax(), val_counts.max() )\n",
    "        self.minority_class = ( val_counts.idxmin(), val_counts.min() )  \n",
    "        \n",
    "        if self.method == 'direct':\n",
    "            \n",
    "            self.base_model.fit(X,y)\n",
    "            pred_prob = self.base_model.predict_proba(X)\n",
    "            pred_prob_minority = pred_prob[:,self.minority_class[0]]\n",
    "            \n",
    "            mean_value = np.mean(pred_prob_minority)\n",
    "            self.threshold = mean_value\n",
    "            if verbose == 'on':\n",
    "                print('method =',self.method)\n",
    "                print('threshold :',mean_value)\n",
    "                \n",
    "        elif self.method == 'ensemble':\n",
    "            if verbose == 'on':\n",
    "                print('method =',self.method)\n",
    "\n",
    "            merged_data_df = pd.concat([X,y],axis=1)\n",
    "\n",
    "            majority_class_df = merged_data_df[merged_data_df[class_col_name] == self.majority_class[0]]\n",
    "            minority_class_df = merged_data_df[merged_data_df[class_col_name] == self.minority_class[0]]\n",
    "            \n",
    "            n_subsets = self.n_ensemble\n",
    "            majority_class_df_split = np.array_split(majority_class_df, n_subsets)\n",
    "            \n",
    "            for i, majority_class_subset in enumerate(majority_class_df_split, 1):\n",
    "                \n",
    "                # ----- Pairing majority class subset with minority class ----- #\n",
    "                paired_dataset = pd.concat( [ majority_class_subset, minority_class_df ], axis=0 )\n",
    "                paired_dataset = paired_dataset.sample(frac=1, random_state=self.random_state) # shuffling\n",
    "                # --------------------------------------------------------------- #\n",
    "                # ----------- For training ensemble model ------------ #\n",
    "                X_train = paired_dataset.drop(class_col_name, axis=1)\n",
    "                y_train = paired_dataset[class_col_name]\n",
    "                # -------------------------------------------------------- #\n",
    "                # ------------------------- Model Training ---------------------- #\n",
    "                self.base_model.fit(X_train, y_train)\n",
    "                # ---------------------------------------------------------------- #\n",
    "                # --- Prediction Probability Extraction for the trained data ---- #\n",
    "                pred_prob = self.base_model.predict_proba(X_train)\n",
    "                pred_prob_minority = pred_prob[:,self.minority_class[0]]\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                # ------------------------- Mean Value Calculation and mapping it to model ------------------------- # \n",
    "                mean_value = np.mean(pred_prob_minority)\n",
    "                self.thres_to_model_mapping[mean_value] = self.base_model\n",
    "            \n",
    "        else :\n",
    "            print('Choose a valid method')\n",
    "            pass\n",
    "\n",
    "    def predict(self,X):\n",
    "        if self.method == 'direct':\n",
    "            pred_prob_minority = self.base_model.predict_proba(X)[:,self.minority_class[0]].tolist()\n",
    "            y_pred = [ self.minority_class[0] if p >= self.threshold else self.majority_class[0] for p in pred_prob_minority ]\n",
    "            return y_pred\n",
    "            \n",
    "        elif self.method == 'ensemble':\n",
    "            pred_df = pd.DataFrame() \n",
    "            for threshold, model in self.thres_to_model_mapping.items():\n",
    "                pred_prob = model.predict_proba(X)\n",
    "                \n",
    "                pred_prob_minority = pred_prob[:,self.minority_class[0]].tolist()\n",
    "                y_pred = [ self.minority_class[0] if p >= threshold else self.majority_class[0] for p in pred_prob_minority ]\n",
    "                \n",
    "                # pred_df[ str(threshold) + '_pred_prob'] = pred_prob_1\n",
    "                pred_df[str(threshold) + '_pred_label'] = y_pred\n",
    "                \n",
    "            # --------------------------------------------- FINAL PREDICTION PROBABILITY CALCULATION --------------------------------------------- #\n",
    "            # columns_to_check = [ c for c in list(pred_df.columns) if 'pred_prob' in c ]\n",
    "            # pred_df['Final_Pred_Prob'] = pred_df[columns_to_check].mean(axis=1)\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------ #\n",
    "            # --------------------------------------------------- FINAL CLASS PREDICTION --------------------------------------------------- #\n",
    "            columns_to_check = [ c for c in list(pred_df.columns) if 'pred_label' in c ]\n",
    "            pred_df['Final Prediction'] =  pred_df[columns_to_check].apply(\n",
    "                lambda row: self.minority_class[0] if (row == self.minority_class[0]).sum() > (row == self.majority_class[0]).sum() \\\n",
    "                                                else self.majority_class[0], axis=1)\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------ #\n",
    "            # pred_df.name = None\n",
    "            output = pred_df['Final Prediction']\n",
    "            output.name = None\n",
    "            \n",
    "            return output\n",
    "            \n",
    "            pass\n",
    "        else : \n",
    "            print('Please pass either \"ensemble\" or \"direct\" in \"method\" parameter ')\n",
    "            pass\n",
    "            \n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        if self.method == 'direct':\n",
    "            pred_probs = self.base_model.predict_proba(X)\n",
    "            return pred_probs\n",
    "        elif self.method == 'ensemble':\n",
    "            pred_df = pd.DataFrame() \n",
    "            for threshold, model in self.thres_to_model_mapping.items():\n",
    "                pred_prob = model.predict_proba(X)\n",
    "                \n",
    "                pred_prob_minority = model.predict_proba(X)[:, self.minority_class[0]].tolist()\n",
    "                pred_prob_majority = model.predict_proba(X)[:, self.majority_class[0]].tolist()\n",
    "                \n",
    "                pred_df[ str(threshold) + '_pred_prob_minority'] = pred_prob_minority\n",
    "                pred_df[ str(threshold) + '_pred_prob_majority'] = pred_prob_majority\n",
    "                \n",
    "            # --------- FINAL PREDICTION PROBABILITY CALCULATION --------- #\n",
    "            if self.minority_class[0] < self.majority_class[0]:\n",
    "                columns_to_check = [ c for c in list(pred_df.columns) if '_pred_prob_minority' in c ]\n",
    "                pred_df['Final_Pred_Prob_minority'] = pred_df[columns_to_check].mean(axis=1)\n",
    "\n",
    "                columns_to_check = [ c for c in list(pred_df.columns) if '_pred_prob_majority' in c ]\n",
    "                pred_df['Final_Pred_Prob_majority'] = pred_df[columns_to_check].mean(axis=1) \n",
    "\n",
    "                return pred_df[[ 'Final_Pred_Prob_minority', 'Final_Pred_Prob_majority' ]].to_numpy()\n",
    "            \n",
    "            else:\n",
    "                columns_to_check = [ c for c in list(pred_df.columns) if '_pred_prob_majority' in c ]\n",
    "                pred_df['Final_Pred_Prob_majority'] = pred_df[columns_to_check].mean(axis=1)         \n",
    "                \n",
    "                columns_to_check = [ c for c in list(pred_df.columns) if '_pred_prob_minority' in c ]\n",
    "                pred_df['Final_Pred_Prob_minority'] = pred_df[columns_to_check].mean(axis=1)\n",
    "            \n",
    "                return pred_df[[ 'Final_Pred_Prob_majority', 'Final_Pred_Prob_minority' ]].to_numpy()\n",
    "        else :\n",
    "            print('Please pass either \"ensemble\" or \"direct\" in \"method\" parameter ')\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8ead5-b4e8-447e-a945-2adf15da22dd",
   "metadata": {},
   "source": [
    "# Initialising Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135b532-dff9-4bdb-b19a-a412a5920a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22307869-4328-4f1a-990a-3e346d0fcdca",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4219747-c91a-4d12-b50e-8ead3ccee34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of parameters\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'learning_rate': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': None,\n",
    "    'random_state': seed,\n",
    "    'use_label_encoder': True\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "#xgb_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e034b7-e055-4fc8-9c3e-002ca5c2de68",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56be7a9-34f1-45e6-bf10-b162d3528e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = dict(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_state=seed,\n",
    "    loss_function='Logloss',\n",
    "    custom_metric=None,\n",
    "    eval_metric=None,\n",
    "    logging_level='Silent',\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    "    od_type='Iter',\n",
    "    od_wait=100,\n",
    "    allow_writing_files=True\n",
    ")\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "#cat_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52811de9-273c-4585-96b2-799c23013c0a",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febffeb1-4f78-4b50-88e4-316ff80bda07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c24bcb-09fc-4bc6-873c-450d9a84e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_standard(classifier,classifier_name,X_train,y_train,performance_df,pkl_dir,pkl_prefix):\n",
    "    # dataframe to store prediction results\n",
    "    train_pred_df = pd.DataFrame()\n",
    "    test_pred_df = pd.DataFrame()    \n",
    "    # ------------------------------------- Standard threshold ------------------------------------- #\n",
    "\n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "    # ----------------- Training Evaluation ------------------- #\n",
    "    \n",
    "    train_pred_df = pd.DataFrame()\n",
    "    train_pred_df['y_true'] = y_train.tolist()\n",
    "    \n",
    "    # predict the training \n",
    "    test_probs = classifier.predict_proba(X_train)[:,1].tolist() #prediction probabilities for the test set\n",
    "    train_pred_df['pred_prob'] = test_probs\n",
    "    \n",
    "    scores = [1 if x>=0.5 else 0 for x in test_probs]\n",
    "    train_pred_df[f'{classifier_name}_standard_pred'] = scores\n",
    "    \n",
    "    performance = dict(dataset = 'training',method = f'{classifier_name}_standard')\n",
    "\n",
    "    train_class_counts = y_train.value_counts()\n",
    "    classes = list(train_class_counts.index)\n",
    "    temp = { idx : train_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "    \n",
    "    temp = dict(threshold = 0.5 )\n",
    "    performance.update(temp)\n",
    "    \n",
    "    y_true = train_pred_df['y_true'].tolist()\n",
    "    y_pred = train_pred_df[f'{classifier_name}_standard_pred'].tolist()\n",
    "    pred_prob = train_pred_df['pred_prob'].tolist()\n",
    "    \n",
    "    \n",
    "    temp = evaluate_performance(\n",
    "    y_true = y_true, \n",
    "    y_pred = y_pred,\n",
    "    pred_prob = pred_prob\n",
    "    )\n",
    "    \n",
    "    performance.update(temp)\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "    \n",
    "    # ----------------- Testing Evaluation ------------------- #\n",
    "    \n",
    "    test_pred_df = pd.DataFrame()\n",
    "    test_pred_df['y_true'] = y_test.tolist()\n",
    "    \n",
    "    # predict the testing \n",
    "    test_probs = classifier.predict_proba(X_test)[:,1].tolist() #prediction probabilities for the test set\n",
    "    test_pred_df['pred_prob'] = test_probs\n",
    "    \n",
    "    scores = [1 if x>=0.5 else 0 for x in test_probs]\n",
    "    test_pred_df[f'{classifier_name}_standard_pred'] = scores\n",
    "    \n",
    "    performance = dict(dataset = 'testing', method = f'{classifier_name}_standard')\n",
    "    \n",
    "    test_class_counts = y_test.value_counts()\n",
    "    classes = list(test_class_counts.index)\n",
    "    temp = { idx : test_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "    \n",
    "    temp = dict(threshold = 0.5 )\n",
    "    performance.update(temp)\n",
    "    \n",
    "    y_true = test_pred_df['y_true'].tolist()\n",
    "    y_pred = test_pred_df[f'{classifier_name}_standard_pred'].tolist()\n",
    "    pred_prob = test_pred_df['pred_prob'].tolist()\n",
    "    \n",
    "    \n",
    "    temp = evaluate_performance(\n",
    "    y_true = y_true, \n",
    "    y_pred = y_pred,\n",
    "    pred_prob = pred_prob\n",
    "    )\n",
    "    \n",
    "    performance.update(temp)\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "\n",
    "    pkl_path = os.path.join(pkl_dir,f'{pkl_prefix}_{classifier_name}_standard.pkl')\n",
    "    # Serialize the object and save it to a file\n",
    "    with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "\n",
    "    return classifier, performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665276d-eb76-4a96-8b71-15b9436800f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GHOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda5bef-a76a-43f0-a8bd-e2cef525c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ghost(classifier,classifier_name,X_train,y_train,performance_df,pkl_dir,pkl_prefix):\n",
    "    # dataframe to store prediction results\n",
    "    train_pred_df = pd.DataFrame()\n",
    "    test_pred_df = pd.DataFrame() \n",
    "    \n",
    "    # ------------------------------------------ GHOST ------------------------------------------------------#\n",
    "    # ---parameters for threshold optimization - we use default values for most parameters\n",
    "    thresholds = np.round(np.arange(0.05,0.55,0.05),2)\n",
    "    random_seed = 42\n",
    "    \n",
    "    #these are default:\n",
    "    ThOpt_metrics = 'Kappa'\n",
    "    N_subsets = 100\n",
    "    subsets_size = 0.2\n",
    "    with_replacement = False\n",
    "\n",
    "    # Can be used for every machine learning model\n",
    "    thresh_sub = optimize_threshold_train_subset(cls=classifier, fps_train=X_train, labels_train=y_train, thresholds=thresholds,\n",
    "                                                              ThOpt_metrics = ThOpt_metrics, \n",
    "                                                              N_subsets = N_subsets, subsets_size = subsets_size, \n",
    "                                                              with_replacement = with_replacement, random_seed = random_seed)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------- ghost threshold----------------------------------#\n",
    "    # ---------------- training evaluation ---------------------#\n",
    "    train_pred_df['y_true'] = y_train.tolist()\n",
    "    pred_probs = classifier.predict_proba(X_train)[:,1].tolist()\n",
    "    train_pred_df['pred_prob'] = pred_probs\n",
    "    \n",
    "    scores = [1 if x>= thresh_sub else 0 for x in train_pred_df['pred_prob'].tolist()]\n",
    "    train_pred_df[f'{classifier_name}_ghost_pred'] = scores\n",
    "    \n",
    "    performance = dict(dataset = 'training',method = f'{classifier_name}_ghost')\n",
    "    \n",
    "    train_class_counts = y_train.value_counts()\n",
    "    classes = list(train_class_counts.index)\n",
    "    temp = { idx : train_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = thresh_sub)\n",
    "    performance.update(temp)\n",
    "    \n",
    "    y_true = train_pred_df['y_true'].tolist()\n",
    "    y_pred = train_pred_df[f'{classifier_name}_ghost_pred'].tolist()\n",
    "    pred_prob = train_pred_df['pred_prob'].tolist()\n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "\n",
    "    performance.update(temp)\n",
    "\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "\n",
    "    # -------------------------------------- ghost threshold -------------------------------------- #\n",
    "    # -------------- testing evaluation ------------------------- #\n",
    "    test_pred_df['y_true'] = y_test.tolist()\n",
    "    pred_probs = classifier.predict_proba(X_test)[:,1].tolist()\n",
    "    test_pred_df['pred_prob'] = pred_probs\n",
    "    \n",
    "    scores = [1 if x>= thresh_sub else 0 for x in test_pred_df['pred_prob'].tolist()]\n",
    "    test_pred_df[f'{classifier_name}_ghost_pred'] = scores\n",
    "    \n",
    "    performance = dict(dataset = 'testing',method = f'{classifier_name}_ghost')\n",
    "    \n",
    "    test_class_counts = y_test.value_counts()\n",
    "    classes = list(test_class_counts.index)\n",
    "    \n",
    "    temp = { idx : test_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = thresh_sub)\n",
    "    performance.update(temp)\n",
    "\n",
    "    y_true = test_pred_df['y_true'].tolist()\n",
    "    y_pred = test_pred_df[f'{classifier_name}_ghost_pred'].tolist()\n",
    "    pred_prob = test_pred_df['pred_prob'].tolist()\n",
    "    \n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "    performance.update(temp)\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "\n",
    "    pkl_path = os.path.join(pkl_dir,f'{pkl_prefix}_{classifier_name}_ghost_{thresh_sub}.pkl')\n",
    "    # Serialize the object and save it to a file\n",
    "    with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "\n",
    "    return performance_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea78040-6bea-4ab6-8117-a60f0813f96f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Direct Mtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8451a-50b0-47a4-8070-89460c0f7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_direct_mtune(classifier,classifier_name,X_train,y_train,performance_df,pkl_dir,pkl_prefix):\n",
    "    train_pred_df = pd.DataFrame()\n",
    "    test_pred_df = pd.DataFrame() \n",
    "    \n",
    "    direct_mtune_model = mtune( classifier, method='direct',random_state = 42)\n",
    "    direct_mtune_model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = direct_mtune_model.predict(X_train)\n",
    "    pred_prob = direct_mtune_model.predict_proba(X_train)\n",
    "    pred_prob_1 = pred_prob[:,1].tolist()\n",
    "\n",
    "    train_pred_df['y_true'] = y_train.tolist()\n",
    "    train_pred_df['pred_prob'] = pred_prob_1\n",
    "    train_pred_df[f'{classifier_name}_direct_mtune_pred'] = y_pred\n",
    "\n",
    "    opt_thres = direct_mtune_model.threshold\n",
    "    \n",
    "    performance = dict(dataset = 'training',method =f'{classifier_name}_direct_mtune')\n",
    "    \n",
    "    train_class_counts = y_train.value_counts()\n",
    "    classes = list(train_class_counts.index)\n",
    "    temp = { idx : train_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = opt_thres)\n",
    "    performance.update(temp)\n",
    "    \n",
    "    y_true = train_pred_df['y_true'].tolist()\n",
    "    y_pred = train_pred_df[f'{classifier_name}_direct_mtune_pred'].tolist()\n",
    "    pred_prob = train_pred_df['pred_prob'].tolist()\n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "\n",
    "    performance.update(temp)\n",
    "\n",
    "    performance_df[performance_df.shape[1]] = performance    \n",
    "\n",
    "\n",
    "    # Testing \n",
    "\n",
    "    y_pred = direct_mtune_model.predict(X_test)\n",
    "    pred_prob = direct_mtune_model.predict_proba(X_test)\n",
    "    pred_prob_1 = pred_prob[:,1].tolist()\n",
    "\n",
    "    test_pred_df['y_true'] = y_test.tolist()\n",
    "    test_pred_df['pred_prob'] = pred_prob_1\n",
    "    test_pred_df[f'{classifier_name}_direct_mtune_pred'] = y_pred\n",
    "    \n",
    "    \n",
    "    performance = dict(dataset = 'testing',method = f'{classifier_name}_direct_mtune')\n",
    "    \n",
    "    test_class_counts = y_test.value_counts()\n",
    "    classes = list(test_class_counts.index)\n",
    "    \n",
    "    temp = { idx : test_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = opt_thres)\n",
    "    performance.update(temp)\n",
    "\n",
    "    y_true = test_pred_df['y_true'].tolist()\n",
    "    y_pred = test_pred_df[f'{classifier_name}_direct_mtune_pred'].tolist()\n",
    "    pred_prob = test_pred_df['pred_prob'].tolist()\n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "    performance.update(temp)\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "\n",
    "    pkl_path = os.path.join(pkl_dir,f'{pkl_prefix}_{classifier_name}_direct_mtune_{opt_thres}.pkl')\n",
    "    # Serialize the object and save it to a file\n",
    "    with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(direct_mtune_model, file)\n",
    "\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533f35b-eb70-4024-802b-96a557b3a93f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ensemble M-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c532fa-09b7-49a9-a428-3e9a4ee7664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ensemble_mtune(classifier,classifier_name,X_train,y_train,performance_df,pkl_dir,pkl_prefix):\n",
    "    train_pred_df = pd.DataFrame()\n",
    "    test_pred_df = pd.DataFrame() \n",
    "    \n",
    "    ensemble_mtune_model = mtune( classifier, method='ensemble',random_state = 42)\n",
    "    ensemble_mtune_model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = ensemble_mtune_model.predict(X_train)\n",
    "    pred_prob = ensemble_mtune_model.predict_proba(X_train)\n",
    "    pred_prob_1 = pred_prob[:,1].tolist()\n",
    "\n",
    "    train_pred_df['y_true'] = y_train.tolist()\n",
    "    train_pred_df['pred_prob'] = pred_prob_1\n",
    "    train_pred_df[f'{classifier_name}_ensemble_mtune_pred'] = y_pred.tolist()\n",
    "    \n",
    "    performance = dict(dataset = 'training',method =f'{classifier_name}_ensemble_mtune')\n",
    "    \n",
    "    train_class_counts = y_train.value_counts()\n",
    "    classes = list(train_class_counts.index)\n",
    "    temp = { idx : train_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = 'mean')\n",
    "    performance.update(temp)\n",
    "    \n",
    "    y_true = train_pred_df['y_true'].tolist()\n",
    "    y_pred = train_pred_df[f'{classifier_name}_ensemble_mtune_pred'].tolist()\n",
    "    pred_prob = train_pred_df['pred_prob'].tolist()\n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "\n",
    "    performance.update(temp)\n",
    "\n",
    "    performance_df[performance_df.shape[1]] = performance    \n",
    "\n",
    "\n",
    "    # Testing \n",
    "\n",
    "    y_pred = ensemble_mtune_model.predict(X_test)\n",
    "    pred_prob = ensemble_mtune_model.predict_proba(X_test)\n",
    "    pred_prob_1 = pred_prob[:,1].tolist()\n",
    "\n",
    "    test_pred_df['y_true'] = y_test.tolist()\n",
    "    test_pred_df['pred_prob'] = pred_prob_1\n",
    "    test_pred_df[f'{classifier_name}_ensemble_mtune_pred'] = y_pred.tolist()\n",
    "    \n",
    "    performance = dict(dataset = 'testing',method = f'{classifier_name}_ensemble_mtune')\n",
    "    \n",
    "    test_class_counts = y_test.value_counts()\n",
    "    classes = list(test_class_counts.index)\n",
    "    \n",
    "    temp = { idx : test_class_counts[idx] for idx in classes }\n",
    "    performance.update(temp)\n",
    "\n",
    "    temp = dict(threshold = 'mean')\n",
    "    performance.update(temp)\n",
    "\n",
    "    y_true = test_pred_df['y_true'].tolist()\n",
    "    y_pred = test_pred_df[f'{classifier_name}_ensemble_mtune_pred'].tolist()\n",
    "    pred_prob = test_pred_df['pred_prob'].tolist()\n",
    "\n",
    "    temp = evaluate_performance(\n",
    "        y_true = y_true, \n",
    "        y_pred = y_pred,\n",
    "        pred_prob = pred_prob\n",
    "    )\n",
    "    performance.update(temp)\n",
    "    performance_df[performance_df.shape[1]] = performance\n",
    "    \n",
    "    pkl_path = os.path.join(pkl_dir,f'{pkl_prefix}_{classifier_name}_ensemble_mtune.pkl')\n",
    "    # Serialize the object and save it to a file\n",
    "    with open(pkl_path, 'wb') as file:\n",
    "        pickle.dump(ensemble_mtune_model, file)\n",
    "        \n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc822ff-6324-41e8-af82-fa02031327c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Function to run all Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751dfa8-fed8-4809-9942-fb9ce7f472fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_all_methods(classifier, classifier_name, X_train, y_train, performance_df, pkl_dir, pkl_prefix):\n",
    "    classifier, performance_df = perform_standard(classifier,classifier_name,X_train,y_train,performance_df, pkl_dir, pkl_prefix)\n",
    "    performance_df = perform_ghost(classifier,classifier_name,X_train,y_train,performance_df, pkl_dir, pkl_prefix)\n",
    "    performance_df = perform_direct_mtune(classifier,classifier_name,X_train,y_train,performance_df, pkl_dir, pkl_prefix)\n",
    "    performance_df = perform_ensemble_mtune(classifier,classifier_name,X_train,y_train,performance_df, pkl_dir, pkl_prefix)\n",
    "\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e3fac-6fa8-4e54-9f9b-d2a67af418d9",
   "metadata": {},
   "source": [
    "# Setting Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87319989-6e89-40d4-8200-81878fd43628",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7ae5b-8610-48bc-9209-8a2388e202e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dir = os.path.join(cwd,'PERFORMANCE')\n",
    "os.makedirs( performance_dir , exist_ok=True )\n",
    "\n",
    "pkl_dir = os.path.join(performance_dir,'pkl_models')\n",
    "os.makedirs( pkl_dir , exist_ok=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3580a60-1b2a-4786-bbb5-4944970e5f0d",
   "metadata": {},
   "source": [
    "Fingerprints for each cell line has to be organized in following order  \n",
    "\n",
    "cell_line_dir  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BJeLR  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AtomPairs2D.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SubstructureCount    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TGF-B  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AtomPairs2D.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SubstructureCount  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe965f-3e8f-4219-8c78-dc5e1f4c6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_dir = 'path/to/cell_line_dir'\n",
    "\n",
    "cell_lines = [ dir for dir in os.listdir(cell_line_dir) if \\\n",
    "     os.path.isdir( os.path.join(cell_line_dir,dir) )\n",
    "]\n",
    "cell_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88853e4e-c9fa-4bdf-a061-05cd399b2c2b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01e54d-5d78-49ed-8cfe-665815e5865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame()\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    print(cell_line)\n",
    "    cell_line_fp_dir = os.path.join(cell_line_dir,cell_line)\n",
    "    for fp in os.listdir(cell_line_fp_dir):\n",
    "        print('\\t',fp)\n",
    "        processed_fp_name = fp.split('.')[0]\n",
    "        processed_fp_name = processed_fp_name.split('_')[0]\n",
    "        \n",
    "        fp_path = os.path.join(cell_line_fp_dir,fp)\n",
    "        \n",
    "        fp_df = pd.read_csv(fp_path)\n",
    "        fp_df = fp_df.rename(columns = {'Activity':'Name'} )\n",
    "\n",
    "        fp_df = fp_df.sample(frac=1, random_state = seed )\n",
    "        train_df,test_df = train_test_split(fp_df, test_size = 0.2, stratify = fp_df['Name'], random_state = seed )\n",
    "        \n",
    "        X_train = train_df.drop('Name', axis=1)\n",
    "        y_train = train_df['Name']\n",
    "    \n",
    "        X_test = test_df.drop('Name', axis=1)\n",
    "        y_test = test_df['Name']\n",
    "\n",
    "        xgb_model = XGBClassifier(**xgb_params)\n",
    "        cat_model = CatBoostClassifier(**cat_params)\n",
    "\n",
    "        temp = pd.DataFrame()\n",
    "\n",
    "\n",
    "        pkl_prefix = f'{cell_line}_{processed_fp_name}'\n",
    "        implement_all_methods(xgb_model,'xgboost',X_train,y_train,temp, pkl_dir,pkl_prefix)\n",
    "        implement_all_methods(cat_model,'catboost',X_train,y_train,temp, pkl_dir,pkl_prefix)\n",
    "        \n",
    "        temp.loc['cell_line'] = cell_line\n",
    "        temp.loc['finger_print'] = processed_fp_name\n",
    "\n",
    "        performance_df = pd.concat([ performance_df , temp ], axis = 1 )\n",
    "\n",
    "performance_df = performance_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662126a-0586-40ea-8377-8530f456f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697ca53-ed45-4152-8e8d-cd47a88d1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_path = os.path.join(performance_dir,'CELL_LINE_PERFORMANCES_MTUNE.xlsx')\n",
    "performance_df.to_excel(op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec2ff7-dfbc-4d33-bd9c-4c462345e379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
